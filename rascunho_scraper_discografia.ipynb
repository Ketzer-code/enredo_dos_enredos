{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# requesting the discography page\n",
    "discography_page = \"https://pt.wikipedia.org/wiki/Discografia_do_Grupo_Especial_do_Rio_de_Janeiro\"\n",
    "page_req = requests.get(discography_page)\n",
    "\n",
    "# creating soup to associate with headers\n",
    "soup = BeautifulSoup(page_req.text, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting all the discography tables\n",
    "discography_tables = soup.find_all(\"table\", {\"class\":\"wikitable\"})\n",
    "\n",
    "# converting all tables to strings to read tables with pandas\n",
    "discography_tables = (str(table) for table in discography_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in such a way that tables that containg multindexes are not multinidexed\n",
    "discography_tables = (\n",
    "    pd.read_html(table, header = 1) if any([\"Lado A\" in table, \"Disco 1\" in table, \"Oficial\" in table]) \n",
    "    else pd.read_html(table) for table in discography_tables\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some tables are returned as lists - selecting the first element of each list\n",
    "discography_tables = (\n",
    "    table[0] if isinstance(table, list) else table for table in discography_tables\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting only columns that we will use\n",
    "discography_tables = [\n",
    "    table.filter(items = [\n",
    "        \"Escola\", \"Enredo\", \"Autor\", \"Intérprete\", \"Intérprete(Participação Especial)\"\n",
    "        ]) for table in discography_tables\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padronizing names\n",
    "for table in discography_tables:\n",
    "    if not table.empty:\n",
    "        table.columns = [\"escola\", \"enredo\", \"autor\", \"interpete\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collecting years\n",
    "discography_years = soup.select(\"h2 span.mw-headline\")\n",
    "\n",
    "# treating list of years to contain only the year\n",
    "discography_years = (str(y) for y in discography_years)\n",
    "discography_years = (y.split(\"\\\"\")[3] for y in discography_years)\n",
    "discography_years = [y for y in discography_years if \"19\" in y or \"20\" in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relating years with respective discography\n",
    "# some years have more than one disk - 1968, 1970, 1971, 1985 and 1988 - multiplying\n",
    "# those to match table list len\n",
    "years_to_duplicate = [1968, 1970, 1971, 1985, 1988]\n",
    "indexes_to_duplicate = ((y - 1968) + 1 for y in years_to_duplicate)\n",
    "years_to_duplicate = (str(y) for y in years_to_duplicate)\n",
    "\n",
    "index_year = dict(zip(indexes_to_duplicate, years_to_duplicate))\n",
    "# creating counter to account for added indexes\n",
    "aux = 0\n",
    "for k, v in index_year.items():\n",
    "    discography_years.insert(k + aux, v + \"_b\")\n",
    "    aux = aux + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to add each year as a column for each dataframe, we need to transform\n",
    "# the list in a list of lists and multiply them for the lenght of the corresponding dataframe\n",
    "year_table = dict(zip(discography_years, discography_tables))\n",
    "discography_years = ([y.replace(\"_b\", \"\")] * len(year_table.get(y)) for y in year_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# picking each list and inserting into each dataframe as a new column\n",
    "for table, year in zip(discography_tables, discography_years):\n",
    "    table.loc[:, \"ano\"] = year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joining dataframe list into a single dataframe\n",
    "discography_df = pd.concat(discography_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping rows that are actually columns from multindex tables BATERIA DAS ESCOLAS Baterias das escolas\n",
    "undesirable_rows = [\n",
    "    \"Lado B\", \"Intérprete\", \"Disco 2\", \n",
    "    \"BATERIA DAS ESCOLAS\", \"Baterias das escolas\"\n",
    "]\n",
    "\n",
    "mask_drop_rows = discography_df.apply(\n",
    "    lambda row: any(\n",
    "        [re.findall(r\"|\".join(undesirable_rows), string) for string in row if isinstance(string, str)]\n",
    "        ), axis = 1\n",
    ")\n",
    "\n",
    "discography_df_filtered = discography_df[~mask_drop_rows]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
